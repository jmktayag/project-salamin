# Post-Interview Feedback Survey

## Overview

This survey is designed to collect actionable feedback from users after they complete a full interview session with Salamin. The survey should take 5-7 minutes to complete and focuses on user experience, value perception, technical performance, and improvement suggestions.

## Survey Structure

### Section 1: Overall Experience (2 questions)

#### Question 1: Overall Satisfaction
**Type:** Rating Scale (1-10)
**Question:** "Overall, how satisfied were you with your interview practice experience today?"

**Scale:**
- 1-2: Very Unsatisfied
- 3-4: Unsatisfied  
- 5-6: Neutral
- 7-8: Satisfied
- 9-10: Very Satisfied

**Follow-up:** If score is 7+, ask for testimonial permission. If score is 6 or below, prioritize their improvement suggestions.

#### Question 2: Value Perception
**Type:** Open-ended
**Question:** "In one sentence, describe the main value you got from this interview practice session."

**Purpose:** Understand perceived value proposition and identify messaging that resonates.

### Section 2: Technical Performance (2 questions)

#### Question 3: Technical Issues
**Type:** Multiple choice + Other
**Question:** "Did you experience any technical issues during your session? (Select all that apply)"

**Options:**
- [ ] No issues - everything worked smoothly
- [ ] Audio playback problems (TTS)
- [ ] Speech recognition not working properly
- [ ] Questions took too long to generate
- [ ] AI feedback seemed generic or unhelpful
- [ ] Interface was confusing or hard to navigate
- [ ] Session crashed or froze
- [ ] Other: ________________

**Purpose:** Identify priority technical improvements.

#### Question 4: Feature Usage
**Type:** Multiple choice
**Question:** "Which features did you use during your session? (Select all that apply)"

**Options:**
- [ ] Text-to-speech (listened to questions)
- [ ] Speech recognition (spoke your answers)
- [ ] Typed responses only
- [ ] Reviewed feedback for each question
- [ ] Completed the full interview analysis
- [ ] Used the progress indicator
- [ ] Tried different interview types/positions

**Purpose:** Understand feature adoption and usage patterns.

### Section 3: Content Quality (1 question)

#### Question 5: AI Feedback Quality
**Type:** Rating Scale (1-10) + Open-ended follow-up
**Question:** "How helpful was the AI feedback you received on your answers?"

**Scale:**
- 1-2: Not helpful at all
- 3-4: Slightly helpful
- 5-6: Moderately helpful
- 7-8: Very helpful
- 9-10: Extremely helpful

**Follow-up:** "What would make the feedback more helpful for you?"

**Purpose:** Assess core value delivery and identify content improvement opportunities.

### Section 4: Likelihood to Recommend (1 question)

#### Question 6: Net Promoter Score
**Type:** Rating Scale (0-10)
**Question:** "How likely are you to recommend this interview practice tool to a friend or colleague who is job searching?"

**Scale:**
- 0-6: Detractors
- 7-8: Passives
- 9-10: Promoters

**Follow-up for Promoters (9-10):** "What specifically would you tell them about this tool?"
**Follow-up for Detractors (0-6):** "What would need to change for you to recommend this tool?"

**Purpose:** Calculate Net Promoter Score and identify advocacy potential.

### Section 5: Improvement Suggestions (1 question)

#### Question 7: Priority Improvements
**Type:** Open-ended
**Question:** "If you could change one thing about this interview practice experience, what would it be?"

**Purpose:** Identify highest-impact improvements from user perspective.

## Optional Demographic Questions (for segmentation)

#### Bonus Question A: Interview Experience
**Type:** Multiple choice
**Question:** "How many job interviews have you had in the past 12 months?"

**Options:**
- 0 interviews
- 1-2 interviews
- 3-5 interviews
- 6-10 interviews
- 10+ interviews

#### Bonus Question B: Career Stage
**Type:** Multiple choice
**Question:** "Which best describes your current situation?"

**Options:**
- Recent graduate (0-2 years experience)
- Early career (2-5 years experience)
- Mid-career (5-10 years experience)
- Senior professional (10+ years experience)
- Career changer/returning to workforce

## Survey Implementation

### Delivery Method
- **Immediate post-session:** Survey link appears after interview completion
- **Email follow-up:** Send survey link via email within 2 hours if not completed immediately
- **Reminder:** One gentle reminder after 24 hours if not completed

### Survey Platform Options
1. **Google Forms** (Free, simple, good analytics)
2. **Typeform** (Better UX, conditional logic, paid)
3. **SurveyMonkey** (Professional features, analytics, paid)
4. **Custom implementation** (Full control, requires development)

### Response Incentives
- **Completion thank you:** "Thank you for helping us improve!"
- **Results sharing:** "We'll share how we're implementing your feedback"
- **Future access:** "You'll be first to know about new features"
- **Optional:** Small gift card for particularly detailed feedback

## Data Analysis Framework

### Quantitative Metrics
- **Overall Satisfaction:** Average score, distribution
- **Net Promoter Score:** Calculate NPS, segment by score
- **AI Feedback Quality:** Average score, correlation with satisfaction
- **Technical Issues:** Frequency of each issue type
- **Feature Usage:** Adoption rates for each feature

### Qualitative Analysis
- **Value Proposition:** Theme analysis of Question 2 responses
- **Improvement Priorities:** Categorize and prioritize suggestions
- **Promoter Advocacy:** Analyze what promoters would tell others
- **Detractor Concerns:** Identify patterns in detractor feedback

### Success Criteria
- **Overall Satisfaction:** 70%+ scoring 7+ (satisfied or very satisfied)
- **Net Promoter Score:** +20 or higher (more promoters than detractors)
- **Technical Issues:** <20% experiencing any technical problems
- **AI Feedback Quality:** 70%+ scoring 7+ (very or extremely helpful)
- **Completion Rate:** 80%+ of participants complete the survey

## Follow-up Actions

### Immediate Actions (Within 48 hours)
- **Thank you message:** Personal thanks to each participant
- **Issue escalation:** Priority fixes for technical issues affecting >2 users
- **Feedback acknowledgment:** Let participants know their input was received

### Short-term Actions (Within 1 week)
- **Analysis compilation:** Summarize findings and identify trends
- **Priority ranking:** Rank improvement suggestions by frequency and impact
- **Development planning:** Plan fixes for high-priority issues

### Long-term Actions (Within 1 month)
- **Implementation updates:** Communicate improvements made based on feedback
- **Results sharing:** Share anonymized insights with participants
- **Relationship building:** Invite participants to future testing rounds

## Survey Questions Summary

1. **Overall Satisfaction** (Rating 1-10)
2. **Value Perception** (Open-ended)
3. **Technical Issues** (Multiple choice)
4. **Feature Usage** (Multiple choice)
5. **AI Feedback Quality** (Rating 1-10 + follow-up)
6. **Net Promoter Score** (Rating 0-10 + follow-up)
7. **Priority Improvements** (Open-ended)

**Estimated completion time:** 5-7 minutes
**Target response rate:** 80%+
**Expected insights:** User satisfaction, technical priorities, feature adoption, improvement roadmap